{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network\n",
    "\n",
    "This notebook explains a possible improvement for the provious model, a resnet50 with dropout. Now we will use a siamese network but initialized of course with the network pretrained in the previous notebook. First, we will construct a confusion matrix from which we will extract hard examples that our network struggles to classify. Second, we will construct the siamese network and train it. Moreover, quite a lot of tools (Datasets, Loss Functions) will need to be created from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import tensorboardX\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import copy\n",
    "import fastai\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Device: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "print(\"Device: \" + str(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a confusion matrix\n",
    "\n",
    "The idea behind siamese networks is that we want to increase the F score by increasing the distance of the feature vectors of samples belonging to different classses and reducing the distance of the feature vectors belonging to the same class. \n",
    "In order to have these pairs, we must save a confusion matrix with the names of the samples and where they were classified. To do this we create a new ImageFolderId dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderId(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        return super(ImageFolderId, self).__getitem__(index), self.imgs[index] #return image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:691: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes C = 45\n",
      "Weights provided to the loss:\n",
      "tensor([0.0280, 0.0333, 0.0167, 0.0148, 0.0022, 0.0369, 0.0429, 0.0391, 0.0072,\n",
      "        0.0116, 0.0108, 0.0202, 0.0166, 0.0097, 0.0249, 0.0179, 0.0121, 0.0397,\n",
      "        0.0182, 0.0236, 0.0181, 0.0470, 0.0130, 0.0248, 0.0473, 0.0210, 0.0114,\n",
      "        0.0349, 0.0233, 0.0261, 0.0078, 0.0054, 0.0217, 0.0293, 0.0420, 0.0045,\n",
      "        0.0116, 0.0482, 0.0098, 0.0274, 0.0141, 0.0220, 0.0045, 0.0416, 0.0169],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/home/user/snakes/train/'\n",
    "validation_dir = '/home/user/snakes/validation/'\n",
    "\n",
    "# Define training parameters \n",
    "size_batch = 4\n",
    "\n",
    "# Like Albert Pumarola said in class, normalize data\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Define datasets\n",
    "# Now the definition of the dataset for the trianing samples is a little bit different than before, we need to know\n",
    "# the class path to build a confusion matrix\n",
    "training_set = ImageFolderId(train_dir, transforms.Compose([transforms.RandomSizedCrop(224),transforms.RandomHorizontalFlip(),transforms.ToTensor(),normalize]))\n",
    "validation_set = torchvision.datasets.ImageFolder(validation_dir, transforms.Compose([transforms.RandomSizedCrop(224),transforms.RandomHorizontalFlip(),transforms.ToTensor(),normalize]))\n",
    "                                    \n",
    "# Define dataloaders                               \n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=size_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(validation_set, batch_size=size_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "C = len(training_set.classes)\n",
    "number_of_training_samples = len(training_set)\n",
    "print(\"Number of classes C = \" + str(C))\n",
    "class_balance = torch.empty(C)\n",
    "i=0\n",
    "for cl in training_set.classes:\n",
    "    # We want to penalize more the classes that are less frequent\n",
    "    class_balance[i] = 1/len(os.listdir(os.path.join(train_dir, cl)))/number_of_training_samples\n",
    "    i += 1\n",
    "\n",
    "normalization_factor = class_balance.sum()\n",
    "class_balance /= normalization_factor\n",
    "class_balance = class_balance.to(device)\n",
    "\n",
    "print(\"Weights provided to the loss:\")\n",
    "print(class_balance)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_balance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the best model with dropout to construct the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): Sequential(\n",
       "        (0): Dropout2d(p=0.15)\n",
       "        (1): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): Sequential(\n",
       "        (0): Dropout2d(p=0.15)\n",
       "        (1): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=2048, out_features=45, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True, progress=True)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Add dropout layers\n",
    "model.layer4[1].relu = nn.Sequential(\n",
    "    nn.Dropout2d(0.15),\n",
    "    nn.ReLU(inplace=True)\n",
    ")\n",
    "model.layer4[2].relu = nn.Sequential(\n",
    "    nn.Dropout2d(0.15),\n",
    "    nn.ReLU(inplace=True)\n",
    ")\n",
    "# Change the last layer to classify 45 classes instead of 1000 and add dropout\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, C)\n",
    ")\n",
    "# Now load the pretrained model with dropout\n",
    "model_weights_path = '/home/user/finetuning/resnet50_snakes_drop_Ep_28_Acc_0.626_F_0.509.pth'\n",
    "model_weights = torch.load(model_weights_path)\n",
    "model.load_state_dict(model_weights)\n",
    "# Now set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we make a forward pass to all the samples in the training set and build the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now make a forward pass through all the training set and build a confusion matrix saving each sample-id\n",
    "model = model.to(device)\n",
    "\n",
    "# The confusion matrix will contain all ids that are missclassified\n",
    "confusion_matrix = [[[] for col in range(45)] for row in range(45)]\n",
    "for inputs, labels in train_loader:\n",
    "    idxs = labels[0]\n",
    "    labels = labels[1]\n",
    "    inputs = inputs[0].to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    preds = preds.cpu()\n",
    "    indices = torch.eq(labels, preds).numpy()\n",
    "    print(\"Labels:\")\n",
    "    print(labels)\n",
    "    print(preds)\n",
    "    wrong_classification = np.where(indices == 0)\n",
    "    correct_classification = np.where(indices == 1)\n",
    "    print(wrong_classification[0])\n",
    "    for i in wrong_classification[0]:\n",
    "        print(\"Sample number \" + str(i)+ \" has been missclassified\")\n",
    "        print(\"It was a sample of class: \" + str(labels[i].item()))\n",
    "        print(\"But it has been classified as: \" + str(preds[i].item()))\n",
    "        print(\"The name of the image is: \" + str(idxs[i]))\n",
    "        confusion_matrix[labels[i]][preds[i].item()].append(idxs[i])\n",
    "    for i in correct_classification[0]:\n",
    "        confusion_matrix[labels[i]][preds[i].item()].append(idxs[i])\n",
    "    break\n",
    "# RUN BELOW LINES TO SAVE THE CONFUSION MATRIX WITH IDs\n",
    "# with open('/home/user/confusion_matrix', 'wb') as f:\n",
    "#     pickle.dump(confusion_matrix, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Siamese\n",
    "\n",
    "### 2.1 Dataset\n",
    "\n",
    "Here is where things get more interesting. To train a siamese network, we need image pairs, so we need to create a PairsDataset class that picks pairs __taking into account the proportion of missclassified samples. Thanks to this, we choose image pairs that our network is likely to confuse__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the confusion matrix is created, a new dataset to sample pairs has to be created\n",
    "class PairsDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, confusion_mat_paths, confusion_mat_counter, transform):\n",
    "        # List of lists (matrix like) containing the ids in the training set\n",
    "        self.confusion_matrix_paths = confusion_mat_paths\n",
    "        # Matrix (C x C) containing the number of classified samples: The rows are the actual class and the columns the \n",
    "        # predicted class. A typical confusion matrix.\n",
    "        self.confusion_matrix_counter = confusion_mat_counter\n",
    "        self.C = 45 # number of classes\n",
    "        self.transform = transform\n",
    "    \n",
    "    # We override __getitem_method\n",
    "    def __getitem__(self,index):\n",
    "        # We'll sample by columns or rows randomly. 0: row, 1: col\n",
    "        row_or_col = np.random.binomial(1,0.5)\n",
    "        if(row_or_col == 0):\n",
    "            # We sample along an entire row (Samples will be of same class) Y = 0\n",
    "            \n",
    "            # We take one class randomly to compare, this could be done with class proportion, but now let us leave it like this\n",
    "            rnd_idx = int(np.random.randint(0, self.C, 1))\n",
    "            # FIRST, take a correctly classified image\n",
    "            correctly_classified_images = self.confusion_matrix_paths[rnd_idx][rnd_idx]\n",
    "            img_0_idx = int(np.random.randint(0,len(correctly_classified_images),1))\n",
    "            img_0_name = correctly_classified_images[img_0_idx]\n",
    "\n",
    "            # SECOND, take a missclassified image\n",
    "            non_null_indices = np.where(self.confusion_matrix_counter[rnd_idx, :] != 0)[0]\n",
    "            row = self.confusion_matrix_counter[rnd_idx, non_null_indices]\n",
    "            proportion_of_predictions = row/np.sum(row)\n",
    "            \n",
    "            choosen_class = rnd_idx\n",
    "            # This while loop assures that a pair is composed of a correctly classified image and a missclassified image\n",
    "            while(choosen_class == rnd_idx):\n",
    "                # We sample taking into account the distribution of our missclassifications\n",
    "                choosen_class = np.random.choice(non_null_indices, p=proportion_of_predictions)\n",
    "            \n",
    "            # Now pick an image from the confusion matrix with image names that we created before\n",
    "            missclassified_images = self.confusion_matrix_paths[rnd_idx][choosen_class]    \n",
    "            number_of_images = len(missclassified_images)\n",
    "            img_1_idx = int(np.random.randint(0, number_of_images, 1))\n",
    "            img_1_name = missclassified_images[img_1_idx]\n",
    "\n",
    "            # Now we have the two images, let us transform them and build a tensor for each one\n",
    "            img0 = Image.open(img_0_name)\n",
    "            img1 = Image.open(img_1_name)\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "            # We return both images and the label (0 because the images belong to the same class)\n",
    "            return img0, img1, torch.from_numpy(np.array([int(0)], dtype=np.float32))\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # We sample along an entire column (Samples will be of different class) Y = 1\n",
    "            # We take one class randomly to compare, this could be done with class proportion, but now let us leave it like this\n",
    "            rnd_idx = int(np.random.randint(0, self.C, 1))\n",
    "            \n",
    "            # FIRST, take a correctly classified image\n",
    "            correctly_classified_images = self.confusion_matrix_paths[rnd_idx][rnd_idx]\n",
    "            img_0_idx = int(np.random.randint(0,len(correctly_classified_images),1))\n",
    "            img_0_name = correctly_classified_images[img_0_idx]\n",
    "\n",
    "            # SECOND, take a missclassified image\n",
    "            non_null_indices = np.where(self.confusion_matrix_counter[:, rnd_idx] != 0)[0]\n",
    "            \n",
    "            col = self.confusion_matrix_counter[non_null_indices, rnd_idx]\n",
    "            proportion_of_predictions = col/np.sum(col)\n",
    "            \n",
    "            choosen_class = rnd_idx\n",
    "            # This while loop assures that a pair is composed of a correctly classified image and a missclassified image\n",
    "            while(choosen_class == rnd_idx):\n",
    "                # We sample taking into account the distribution of our missclassifications\n",
    "                choosen_class = np.random.choice(non_null_indices, p=proportion_of_predictions)\n",
    "                \n",
    "            # Now pick an image from the confusion matrix with image names that we created before\n",
    "            missclassified_images = self.confusion_matrix_paths[choosen_class][rnd_idx]    \n",
    "            number_of_images = len(missclassified_images)\n",
    "            img_1_idx = int(np.random.randint(0, number_of_images, 1))\n",
    "            img_1_name = missclassified_images[img_1_idx]\n",
    "            \n",
    "            # Now we have the two images, let us transform them and build a tensor for each one\n",
    "            img0 = Image.open(img_0_name)\n",
    "            img1 = Image.open(img_1_name)\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "            # We return the images and a 1 as a label because images belong to different classes\n",
    "            return img0, img1, torch.from_numpy(np.array([int(1)], dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        number_of_pairs = 0\n",
    "        for i in range(0, np.shape(self.confusion_matrix_counter)[0]):\n",
    "            number_of_pairs_in_row = self.confusion_matrix_counter[i,i] * (np.sum(self.confusion_matrix_counter[i, :]) - self.confusion_matrix_counter[i,i])\n",
    "            number_of_pairs_in_col = self.confusion_matrix_counter[i,i] * (np.sum(self.confusion_matrix_counter[:, i]) - self.confusion_matrix_counter[i,i])\n",
    "            number_of_pairs += 0.5 * number_of_pairs_in_col + 0.5 * number_of_pairs_in_row\n",
    "        return int(number_of_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now load the confusion matrix\n",
    "with open('/home/user/confusion_matrix','rb') as f:\n",
    "    confusion_matrix = pickle.load(f)\n",
    "cm = np.zeros((45,45))\n",
    "for i in range(45):\n",
    "        for j in range(45):\n",
    "                cm[i,j] = len(confusion_matrix[i][j])\n",
    "# Dataset and dataloader\n",
    "training_set_pairs = PairsDataset(confusion_matrix, cm,transforms.Compose([transforms.RandomSizedCrop(224),transforms.RandomHorizontalFlip(),transforms.ToTensor(),normalize]))\n",
    "dataloader_pairs = torch.utils.data.DataLoader(training_set_pairs, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Network\n",
    "\n",
    "The important thing to notice is that we are passing as argument a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # remove the last layer of resnet (We want only the Feature vector)\n",
    "        self.cnn_no_fc = torch.nn.Sequential(*(list(pretrained_model.children())[:-1]))\n",
    "        print(self.cnn_no_fc)\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn_no_fc(x)\n",
    "        return output\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "    def save_model_weights(self, i):\n",
    "        torch.save(self.cnn_no_fc.state_dict(), '/home/user/siamese/resnet50_snakes_siamese_{}.pth'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Sequential(\n",
      "        (0): Dropout2d(p=0.15)\n",
      "        (1): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Sequential(\n",
      "        (0): Dropout2d(p=0.15)\n",
      "        (1): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "siamese_net = SiameseNetwork(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loss Function\n",
    "\n",
    "I used the contrastive loss function proposed by Hadsell, Chopra and LeCun in http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_siamese = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(siamese_net.parameters(),lr = 0.0005 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(0,num_epochs):\n",
    "    for i, data in enumerate(dataloader_pairs,0):\n",
    "        img0, img1 , label = data\n",
    "        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n",
    "        print(img0.size())\n",
    "        print(label)\n",
    "        optimizer.zero_grad()\n",
    "        output1,output2 = siamese_net(img0,img1)\n",
    "        print(output1.size())\n",
    "        loss_contrastive = criterion_siamese(output1,output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n",
    "        if i%30 == 0:\n",
    "            siamese_net.save_model_weights(i)\n",
    "        # REMOVE THIS BREAK TO TRAIN\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 INFERENCE ON VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features = 2048\n",
      "20614\n",
      "Siamese --> Acc: 0.4821 F: 0.360\n"
     ]
    }
   ],
   "source": [
    "siamese_weights_path = '/home/user/siamese/resnet50_snakes_siamese_0.pth'\n",
    "dropout_weights_path = '/home/user/finetuning/resnet50_snakes_drop_Ep_28_Acc_0.626_F_0.509.pth'\n",
    "\n",
    "model_dropout = torchvision.models.resnet50(pretrained=True, progress=True)\n",
    "num_ftrs = model_dropout.fc.in_features\n",
    "print(\"Num features = \" + str(num_ftrs))\n",
    "# Add dropout layers\n",
    "model_dropout.layer4[1].relu = nn.Sequential(\n",
    "    nn.Dropout2d(0.15),\n",
    "    nn.ReLU(inplace=True)\n",
    ")\n",
    "model_dropout.layer4[2].relu = nn.Sequential(\n",
    "    nn.Dropout2d(0.15),\n",
    "    nn.ReLU(inplace=True)\n",
    ")\n",
    "#Change the last layer to classify 45 classes instead of 1000 and add dropout\n",
    "model_dropout.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, C)\n",
    ")\n",
    "\n",
    "# Load siamese weights into our model with dropout\n",
    "dropout_dict = torch.load(dropout_weights_path)\n",
    "siamese_dict = torch.load(siamese_weights_path)\n",
    "siamese_dict_keys = list(siamese_dict.keys())\n",
    "count = 0\n",
    "# We have to copy the siamese weights to dropout net\n",
    "for k in dropout_dict.keys():\n",
    "    if(count > len(siamese_dict_keys)):\n",
    "        dropout_dict[k] = siamese_dict[siamese_dict_keys[count]]\n",
    "        count += 1\n",
    "\n",
    "\n",
    "model_dropout.load_state_dict(dropout_dict)\n",
    "\n",
    "model_dropout = model_dropout.to(device)\n",
    "f_score_list = []\n",
    "print(len(val_loader.dataset))\n",
    "running_corrects = 0\n",
    "for inputs, labels in val_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # zero the parameter gradients\n",
    "    # forward\n",
    "    # track history if only in train\n",
    "    # Get model outputs and calculate\n",
    "    outputs = model_dropout(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # backward + optimize only if in training phase\n",
    "    # statistics\n",
    "    # Compute F1-score\n",
    "    labels_cpu = labels.cpu().numpy()\n",
    "    predictions_cpu = preds.cpu().numpy()\n",
    "    F_score = f1_score(labels_cpu, predictions_cpu, average='macro')\n",
    "    f_score_list.append(F_score)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "epoch_f_score = np.average(np.array(f_score_list))\n",
    "print('Siamese --> Acc: {:.4f} F: {:.3f}'.format(epoch_acc, epoch_f_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdf7e59c5c0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
